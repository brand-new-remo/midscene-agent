"""
LangGraph Agent ä¸ Midscene é›†æˆ

æœ¬æ¨¡å—å®ç°äº†ä¸€ä¸ªåŸºäº LangGraph çš„æ™ºèƒ½ä½“ï¼Œä½¿ç”¨ DeepSeek LLM
è¿›è¡Œæ¨ç†ï¼Œä½¿ç”¨ Midscene è¿›è¡Œç½‘é¡µè‡ªåŠ¨åŒ–ã€‚
"""

from typing import List, Dict, Any, Optional, AsyncGenerator
from langchain_core.tools import BaseTool
from langchain_deepseek import ChatDeepSeek
from langchain_core.messages import HumanMessage
from langgraph.graph import StateGraph, MessagesState, START, END
from pydantic import SecretStr
from .mcp_wrapper import MidsceneMCPWrapper


class MidsceneAgent:
    """
    ç”¨äº AI é©±åŠ¨ç½‘é¡µè‡ªåŠ¨åŒ–çš„ LangGraph Agentã€‚

    è¯¥æ™ºèƒ½ä½“ç»“åˆäº†ï¼š
    - DeepSeek LLM ç”¨äºæ¨ç†å’Œå†³ç­–
    - Midscene ç”¨äºè§†è§‰é©±åŠ¨çš„ç½‘é¡µäº¤äº’
    - LangGraph ç”¨äºçŠ¶æ€ç®¡ç†å’Œæ‰§è¡Œæµç¨‹
    """

    def __init__(
        self,
        deepseek_api_key: str,
        deepseek_base_url: str = "https://api.deepseek.com/v1",
        deepseek_model: str = "deepseek-chat",
        temperature: float = 0,
        midscene_command: str = "npx",
        midscene_args: Optional[List[str]] = None,
        env: Optional[Dict[str, Any]] = None,
        tool_set: str = "full",
    ):
        """
        åˆå§‹åŒ– Midscene æ™ºèƒ½ä½“ã€‚

        Args:
            deepseek_api_key: DeepSeek çš„ API å¯†é’¥
            deepseek_base_url: DeepSeek API çš„åŸºç¡€ URL
            deepseek_model: è¦ä½¿ç”¨çš„æ¨¡å‹åç§°
            temperature: LLM å“åº”çš„æ¸©åº¦å‚æ•°
            midscene_command: è¿è¡Œ Midscene MCP æœåŠ¡å™¨çš„å‘½ä»¤
            midscene_args: Midscene å‘½ä»¤çš„å‚æ•°
            env: ç¯å¢ƒå˜é‡
            tool_set: å·¥å…·é›†é€‰æ‹©ï¼š'basic'ï¼ˆåŸºç¡€ï¼‰ã€'advanced'ï¼ˆé«˜çº§ï¼‰ã€'full'ï¼ˆå®Œæ•´ï¼‰
        """
        self.deepseek_api_key = deepseek_api_key
        self.deepseek_base_url = deepseek_base_url
        self.deepseek_model = deepseek_model
        self.temperature = temperature
        self.tool_set = tool_set

        self.mcp_wrapper = MidsceneMCPWrapper(
            midscene_command=midscene_command, midscene_args=midscene_args, env=env
        )

        self.llm: Optional[Any] = None
        self.agent_executor: Optional[Any] = None

    async def initialize(self) -> None:
        """
        åˆå§‹åŒ– LLM å’Œæ™ºèƒ½ä½“æ‰§è¡Œå™¨ã€‚

        Raises:
            RuntimeError: å¦‚æœåˆå§‹åŒ–å¤±è´¥
        """
        try:
            # åˆå§‹åŒ– MCP è¿æ¥
            await self.mcp_wrapper.start()

            # ä½¿ç”¨æ–°çš„å·¥å…·ç³»ç»Ÿè·å–å·¥å…·
            print(f"\nğŸ”§ æ­£åœ¨åˆ›å»ºå·¥å…·é›†: {self.tool_set}")
            tools = await self.mcp_wrapper.get_langchain_tools(tool_set=self.tool_set)
            print(f"âœ… ä¸ºæ™ºèƒ½ä½“åˆ›å»ºäº† {len(tools)} ä¸ªå·¥å…·")

            # åˆå§‹åŒ– LLMï¼ˆç»‘å®šå·¥å…·ï¼‰
            self.llm = ChatDeepSeek(
                model=self.deepseek_model,
                api_key=SecretStr(self.deepseek_api_key),
                base_url=self.deepseek_base_url,
                temperature=self.temperature,
                streaming=True,
            ).bind_tools(tools)

            print(f"\nâœ… å·²åˆå§‹åŒ– DeepSeek LLM ({self.deepseek_model}) å¹¶ç»‘å®š {len(tools)} ä¸ªå·¥å…·")

            # ä½¿ç”¨ StateGraph åˆ›å»ºæ™ºèƒ½ä½“æ‰§è¡Œå™¨
            from langgraph.prebuilt import ToolNode, tools_condition

            # æ„å»ºæ™ºèƒ½ä½“å›¾
            def agent_node(state: MessagesState) -> MessagesState:
                if self.llm is None:
                    raise RuntimeError("LLM æœªåˆå§‹åŒ–")

                # ç®€åŒ–çš„æ—¥å¿—è¾“å‡ºï¼šåªæ˜¾ç¤ºæ¶ˆæ¯æ•°é‡å’Œå·¥å…·è°ƒç”¨
                num_messages = len(state['messages'])
                # print(f"ğŸ¤– Agent Node: {num_messages} messages")

                response = self.llm.invoke(state["messages"])

                # åªåœ¨æœ‰å·¥å…·è°ƒç”¨æ—¶æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯
                if hasattr(response, "tool_calls") and response.tool_calls:
                    print(f"ğŸ’¬ LLM Response: {response.content}")
                    # print(f"ğŸ”§ Tool calls: {len(response.tool_calls)}")
                elif hasattr(response, "content") and response.content:
                    # æ˜¾ç¤ºéå·¥å…·è°ƒç”¨çš„å“åº”å†…å®¹ï¼ˆæˆªæ–­ï¼‰
                    content = str(response.content)
                    if len(content) > 100:
                        print(f"ğŸ’¬ LLM Response: {content[:100]}...")
                    else:
                        print(f"ğŸ’¬ LLM Response: {content}")

                return {"messages": state["messages"] + [response]}

            # åˆ›å»ºå›¾
            builder = StateGraph(MessagesState)
            builder.add_node("agent", agent_node)
            builder.add_node("tools", ToolNode(tools))
            builder.add_edge(START, "agent")
            builder.add_conditional_edges(
                "agent", tools_condition, {"tools": "tools", "__end__": END}
            )
            builder.add_edge("tools", "agent")

            self.agent_executor = builder.compile(
                interrupt_before=[], interrupt_after=[]  # å¯é€‰ï¼šä¸­æ–­ç‚¹  # å¯é€‰ï¼šä¸­æ–­ç‚¹
            )
            print("âœ… æ™ºèƒ½ä½“æ‰§è¡Œå™¨å·²åˆå§‹åŒ–")

        except Exception as e:
            await self.cleanup()
            raise RuntimeError(f"åˆå§‹åŒ–æ™ºèƒ½ä½“å¤±è´¥: {e}")

    async def execute(self, user_input: str, stream: bool = True) -> AsyncGenerator:
        """
        ä½¿ç”¨æ™ºèƒ½ä½“æ‰§è¡Œä»»åŠ¡ã€‚

        Args:
            user_input: ä»»åŠ¡çš„è‡ªç„¶è¯­è¨€æŒ‡ä»¤
            stream: æ˜¯å¦æµå¼ä¼ è¾“å“åº”

        Yields:
            æ™ºèƒ½ä½“æ‰§è¡Œçš„äº‹ä»¶

        Raises:
            RuntimeError: å¦‚æœæ™ºèƒ½ä½“æœªåˆå§‹åŒ–
        """
        if not self.agent_executor:
            raise RuntimeError("æ™ºèƒ½ä½“æœªåˆå§‹åŒ–ã€‚è¯·å…ˆè°ƒç”¨ initialize()ã€‚")

        print(f"\nğŸš€ å¼€å§‹æ‰§è¡Œæ™ºèƒ½ä½“")
        print(f"ğŸ“ ä»»åŠ¡: {user_input}\n")

        try:
            # ä¸º LangChain 1.0+ ä½¿ç”¨ HumanMessage
            input_messages = {"messages": [HumanMessage(content=user_input)]}

            # é…ç½®æœ€å¤§é€’å½’æ¬¡æ•°ä»¥é¿å…å¾ªç¯
            config = {"recursion_limit": 100}

            if stream:
                async for chunk in self.agent_executor.astream(
                    input_messages, config=config
                ):
                    # Yield each chunk as an event
                    yield chunk
            else:
                result = await self.agent_executor.ainvoke(
                    input_messages, config=config
                )
                yield result
        except Exception as e:
            import traceback

            yield {"error": str(e), "traceback": traceback.format_exc()}

    async def cleanup(self) -> None:
        """æ¸…ç†èµ„æºã€‚"""
        if self.mcp_wrapper:
            await self.mcp_wrapper.stop()

    async def __aenter__(self):
        """å¼‚æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨å…¥å£ã€‚"""
        await self.initialize()
        return self

    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """å¼‚æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨å‡ºå£ã€‚"""
        await self.cleanup()
