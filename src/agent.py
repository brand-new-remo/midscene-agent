"""
LangGraph Agent ä¸ Midscene é›†æˆ

æœ¬æ¨¡å—å®ç°äº†ä¸€ä¸ªåŸºäº LangGraph çš„æ™ºèƒ½ä½“ï¼Œä½¿ç”¨ DeepSeek LLM
è¿›è¡Œæ¨ç†ï¼Œä½¿ç”¨ Midscene è¿›è¡Œç½‘é¡µè‡ªåŠ¨åŒ–ã€‚
"""

import asyncio
import sys
from typing import List, Dict, Any, Optional, AsyncGenerator, Literal
from langchain_core.tools import BaseTool, tool
from langchain_deepseek import ChatDeepSeek
from langchain_core.messages import HumanMessage, SystemMessage, AIMessage
from langgraph.graph import StateGraph, MessagesState, START, END
from pydantic import SecretStr

# Import mcp_wrapper from the same directory
from mcp_wrapper import MidsceneMCPWrapper


def create_midscene_action_tool(mcp_wrapper: MidsceneMCPWrapper) -> BaseTool:
    """
    ä¸º Midscene æ“ä½œæ‰§è¡Œåˆ›å»º LangChain å·¥å…·ã€‚

    Args:
        mcp_wrapper: Midscene MCP åŒ…è£…å™¨å®ä¾‹

    Returns:
        ç”¨äºæ‰§è¡Œç½‘é¡µæ“ä½œçš„ LangChain BaseTool
    """

    @tool
    async def midscene_action(instruction: str) -> str:
        """
        ä½¿ç”¨ Midscene çš„ AI èƒ½åŠ›æ‰§è¡Œæµè§ˆå™¨æ“ä½œã€‚

        è¯¥å·¥å…·å…è®¸ä½¿ç”¨è‡ªç„¶è¯­è¨€æŒ‡ä»¤ä¸ç½‘é¡µè¿›è¡Œäº¤äº’ã€‚
        Midscene å°†åˆ†æé¡µé¢çŠ¶æ€å¹¶æ‰§è¡Œè¯·æ±‚çš„æ“ä½œã€‚

        Args:
            instruction: è¦æ‰§è¡Œçš„æ¸…æ™°è‡ªç„¶è¯­è¨€æè¿°ã€‚
                        ç¤ºä¾‹ï¼š
                        - "ç‚¹å‡»ç™»å½•æŒ‰é’®"
                        - "åœ¨æœç´¢æ¡†ä¸­è¾“å…¥ 'hello world'"
                        - "å‘ä¸‹æ»šåŠ¨æŸ¥çœ‹æ›´å¤šå†…å®¹"
                        - "å¯¼èˆªåˆ° https://www.google.com"
                        - "å¡«å†™è¡¨å• name='John Doe' å’Œ email='john@example.com'"

        Returns:
            è¯¦ç»†æè¿°æ‰§è¡Œå†…å®¹å’Œè§‚å¯Ÿç»“æœ
        """
        try:
            result = await mcp_wrapper.call_tool("action", {"instruction": instruction})
            if hasattr(result, 'content') and result.content:
                # Extract text from TextContent array
                content = result.content
                if isinstance(content, list) and len(content) > 0:
                    first_item = content[0]
                    if hasattr(first_item, 'text'):
                        return first_item.text
                    else:
                        return str(first_item)
                else:
                    return str(content)
            return "æ“ä½œæ‰§è¡ŒæˆåŠŸ"
        except Exception as e:
            return f"æ‰§è¡Œæ“ä½œæ—¶å‡ºé”™: {str(e)}"

    return midscene_action


def create_midscene_query_tool(mcp_wrapper: MidsceneMCPWrapper) -> BaseTool:
    """
    ä¸º Midscene æŸ¥è¯¢å’Œä¿¡æ¯æå–åˆ›å»º LangChain å·¥å…·ã€‚

    Args:
        mcp_wrapper: Midscene MCP åŒ…è£…å™¨å®ä¾‹

    Returns:
        ç”¨äºæŸ¥è¯¢é¡µé¢ä¿¡æ¯çš„ LangChain BaseTool
    """

    @tool
    async def midscene_query(question: str) -> str:
        """
        ä½¿ç”¨ Midscene çš„ AI ä»å½“å‰ç½‘é¡µæå–ä¿¡æ¯ã€‚

        è¯¢é—®é¡µé¢ä¸Šå¯è§çš„å†…å®¹ï¼ŒMidscene å°†åˆ†ææˆªå›¾å¹¶æä¾›ç­”æ¡ˆã€‚

        Args:
            question: å…³äºé¡µé¢å†…å®¹çš„é—®é¢˜ã€‚
                     ç¤ºä¾‹ï¼š
                     - "è¿™ä¸ªé¡µé¢çš„æ ‡é¢˜æ˜¯ä»€ä¹ˆï¼Ÿ"
                     - "åˆ—å‡ºæ‰€æœ‰å¯¼èˆªèœå•é¡¹"
                     - "æ˜¾ç¤ºçš„äº§å“ä»·æ ¼æ˜¯å¤šå°‘ï¼Ÿ"
                     - "ä»é¡µé¢ä¸­æå–è”ç³»ä¿¡æ¯"
                     - "é¡µé¢ä¸Šå¯è§å“ªäº›æŒ‰é’®æˆ–é“¾æ¥ï¼Ÿ"

        Returns:
            æå–çš„ä¿¡æ¯æˆ–é—®é¢˜çš„ç­”æ¡ˆ
        """
        try:
            result = await mcp_wrapper.call_tool("query", {"question": question})
            if hasattr(result, 'content') and result.content:
                # Extract text from TextContent array
                content = result.content
                if isinstance(content, list) and len(content) > 0:
                    first_item = content[0]
                    if hasattr(first_item, 'text'):
                        return first_item.text
                    else:
                        return str(first_item)
                else:
                    return str(content)
            return "æŸ¥è¯¢æ‰§è¡ŒæˆåŠŸ"
        except Exception as e:
            return f"æ‰§è¡ŒæŸ¥è¯¢æ—¶å‡ºé”™: {str(e)}"

    return midscene_query


class MidsceneAgent:
    """
    ç”¨äº AI é©±åŠ¨ç½‘é¡µè‡ªåŠ¨åŒ–çš„ LangGraph Agentã€‚

    è¯¥æ™ºèƒ½ä½“ç»“åˆäº†ï¼š
    - DeepSeek LLM ç”¨äºæ¨ç†å’Œå†³ç­–
    - Midscene ç”¨äºè§†è§‰é©±åŠ¨çš„ç½‘é¡µäº¤äº’
    - LangGraph ç”¨äºçŠ¶æ€ç®¡ç†å’Œæ‰§è¡Œæµç¨‹
    """

    def __init__(
        self,
        deepseek_api_key: str,
        deepseek_base_url: str = "https://api.deepseek.com/v1",
        deepseek_model: str = "deepseek-chat",
        temperature: float = 0,
        midscene_command: str = "npx",
        midscene_args: Optional[List[str]] = None,
        env: Optional[Dict[str, Any]] = None
    ):
        """
        åˆå§‹åŒ– Midscene æ™ºèƒ½ä½“ã€‚

        Args:
            deepseek_api_key: DeepSeek çš„ API å¯†é’¥
            deepseek_base_url: DeepSeek API çš„åŸºç¡€ URL
            deepseek_model: è¦ä½¿ç”¨çš„æ¨¡å‹åç§°
            temperature: LLM å“åº”çš„æ¸©åº¦å‚æ•°
            midscene_command: è¿è¡Œ Midscene MCP æœåŠ¡å™¨çš„å‘½ä»¤
            midscene_args: Midscene å‘½ä»¤çš„å‚æ•°
            env: ç¯å¢ƒå˜é‡
        """
        self.deepseek_api_key = deepseek_api_key
        self.deepseek_base_url = deepseek_base_url
        self.deepseek_model = deepseek_model
        self.temperature = temperature

        self.mcp_wrapper = MidsceneMCPWrapper(
            midscene_command=midscene_command,
            midscene_args=midscene_args,
            env=env
        )

        self.llm: Optional[Any] = None
        self.agent_executor: Optional[Any] = None

    async def initialize(self) -> None:
        """
        åˆå§‹åŒ– LLM å’Œæ™ºèƒ½ä½“æ‰§è¡Œå™¨ã€‚

        Raises:
            RuntimeError: å¦‚æœåˆå§‹åŒ–å¤±è´¥
        """
        try:
            # åˆå§‹åŒ– MCP è¿æ¥
            await self.mcp_wrapper.start()

            # åˆ›å»ºå·¥å…·
            tools = [
                create_midscene_action_tool(self.mcp_wrapper),
                create_midscene_query_tool(self.mcp_wrapper)
            ]
            print(f"ğŸ”§ ä¸ºæ™ºèƒ½ä½“åˆ›å»ºäº† {len(tools)} ä¸ªå·¥å…·")

            # åˆå§‹åŒ– LLMï¼ˆç»‘å®šå·¥å…·ï¼‰
            self.llm = ChatDeepSeek(
                model=self.deepseek_model,
                api_key=SecretStr(self.deepseek_api_key),
                base_url=self.deepseek_base_url,
                temperature=self.temperature,
                streaming=True
            ).bind_tools(tools)

            print(f"âœ… å·²åˆå§‹åŒ– DeepSeek LLM ({self.deepseek_model}) å¹¶ç»‘å®šå·¥å…·")

            # ä½¿ç”¨ StateGraph åˆ›å»ºæ™ºèƒ½ä½“æ‰§è¡Œå™¨
            from langgraph.prebuilt import ToolNode, tools_condition

            # æ„å»ºæ™ºèƒ½ä½“å›¾
            def agent_node(state: MessagesState) -> MessagesState:
                if self.llm is None:
                    raise RuntimeError("LLM æœªåˆå§‹åŒ–")
                print(f"\nğŸ¤– Agent Node: Processing {len(state['messages'])} messages")
                for i, msg in enumerate(state["messages"]):
                    print(f"  Message {i}: {type(msg).__name__}")
                    if hasattr(msg, 'content'):
                        content = str(msg.content)[:100]
                        print(f"    Content: {content}...")
                response = self.llm.invoke(state["messages"])
                print(f"\nğŸ’¬ LLM Response: {type(response).__name__}")
                if hasattr(response, 'content'):
                    print(f"  Content: {response.content}")
                if hasattr(response, 'tool_calls'):
                    print(f"  Tool calls: {len(response.tool_calls) if response.tool_calls else 0}")
                return {"messages": state["messages"] + [response]}

            # åˆ›å»ºå›¾
            builder = StateGraph(MessagesState)
            builder.add_node("agent", agent_node)
            builder.add_node("tools", ToolNode(tools))
            builder.add_edge(START, "agent")
            builder.add_conditional_edges(
                "agent",
                tools_condition,
                {"tools": "tools", "__end__": END}
            )
            builder.add_edge("tools", "agent")

            self.agent_executor = builder.compile()
            print("âœ… æ™ºèƒ½ä½“æ‰§è¡Œå™¨å·²åˆå§‹åŒ–")

        except Exception as e:
            await self.cleanup()
            raise RuntimeError(f"åˆå§‹åŒ–æ™ºèƒ½ä½“å¤±è´¥: {e}")

    async def execute(self, user_input: str, stream: bool = True) -> AsyncGenerator:
        """
        ä½¿ç”¨æ™ºèƒ½ä½“æ‰§è¡Œä»»åŠ¡ã€‚

        Args:
            user_input: ä»»åŠ¡çš„è‡ªç„¶è¯­è¨€æŒ‡ä»¤
            stream: æ˜¯å¦æµå¼ä¼ è¾“å“åº”

        Yields:
            æ™ºèƒ½ä½“æ‰§è¡Œçš„äº‹ä»¶

        Raises:
            RuntimeError: å¦‚æœæ™ºèƒ½ä½“æœªåˆå§‹åŒ–
        """
        if not self.agent_executor:
            raise RuntimeError("æ™ºèƒ½ä½“æœªåˆå§‹åŒ–ã€‚è¯·å…ˆè°ƒç”¨ initialize()ã€‚")

        print(f"\nğŸš€ å¼€å§‹æ‰§è¡Œæ™ºèƒ½ä½“")
        print(f"ğŸ“ ä»»åŠ¡: {user_input}\n")

        try:
            # ä¸º LangChain 1.0+ ä½¿ç”¨ HumanMessage
            input_messages = {"messages": [HumanMessage(content=user_input)]}

            if stream:
                async for chunk in self.agent_executor.astream(input_messages):
                    # Yield each chunk as an event
                    yield chunk
            else:
                result = await self.agent_executor.ainvoke(input_messages)
                yield result
        except Exception as e:
            import traceback
            yield {"error": str(e), "traceback": traceback.format_exc()}

    async def cleanup(self) -> None:
        """æ¸…ç†èµ„æºã€‚"""
        if self.mcp_wrapper:
            await self.mcp_wrapper.stop()

    async def __aenter__(self):
        """å¼‚æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨å…¥å£ã€‚"""
        await self.initialize()
        return self

    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """å¼‚æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨å‡ºå£ã€‚"""
        await self.cleanup()

